# Training and Validating on Urban Flood Observations (UFO)

This repository contains the code for training models on UFO and evaluating other models on UFO. UFO contains 231 hand-labels on 14 global flood events.  A U-Net architecture with a **RexNet_150** encoder (as detailed in the associated paper) is defined in this repository. The pipeline for training a model on UFO includes the code for training with Leave-One-Event-Out cross-validation, and prediction, quantitative evaluation, and scripts/notebook cells for generating analysis figures.

This code is associated with the research paper:
**Urban Flood Observations (UFO): A hand-labeled training and validation dataset of post-flood inundation**
*Rohit Mukherjee, Hannah K. Friedrich, Beth Tellman, Ariful Islam, Zhijie Zhang, Jonathan Giezendanner, Upmanu Lall, and Venkataraman Lakshmi*

**Note:** The code contains hardcoded file paths that must be updated to match your local setup after downloading and extracting the dataset. The paths in the scripts are configured for a specific directory structure, likely corresponding to the file organization on the system used for development.

## Dataset

The Urban Flood Observation (UFO) dataset, a global, high-resolution, hand-labeled collection of post-flood inundation imagery, is the core data source for this project. It is publicly available on Zenodo:

**Dataset Link:** [https://zenodo.org/records/15238470](https://zenodo.org/records/15238470)

Please download and extract the dataset. The codebase is configured to expect a specific directory structure derived from the extracted data, particularly for the input images and ground truth labels used for training and validation.

Additionally, some scripts or notebook cells might require specific CSV files generated by the evaluation steps (`predictionsPSS1.csv`, `predictionsPSDW.csv`, `perLabelSpec.csv`, etc.) or the GeoJSON file containing event locations (`UFOEventsCentroids.geojson`) mentioned in the paper. Ensure these are accessible and their paths are updated accordingly.

## Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_name>
    ```
2.  **Install dependencies:** It is highly recommended to use a virtual environment (`conda` or `venv`). The required libraries are listed in the paper's Code Availability section and inferred from the code imports.
    ```bash
    pip install pandas numpy torch torchvision torchaudio fastai rasterio scikit-image scikit-learn seaborn matplotlib geopandas cartopy squarify timm
    ```
    *Note: GPU acceleration is essential for timely training of the deep learning model with PyTorch/fastai. The provided `singlePipeline_timmModels.py` script explicitly sets the device to CUDA device 1 (`torch.cuda.set_device(1)`), which may need adjustment based on your system's GPU configuration.*

## Code Structure and Usage

The repository is organized into key Python scripts and a Jupyter Notebook, each serving a specific part of the workflow or analysis:

1.  **`singlePipeline_timmModels.py`:**
    *   **Purpose:** Executes the full end-to-end workflow: **training → inference → mosaicking → evaluation**.
    *   **Functionality:** Implements the Leave-One-Event-Out cross-validation strategy using a U-Net model with a **RexNet_150** encoder loaded via the `timm` library. It iteratively trains the model on 13 flood events, uses the trained model to predict inundation for the held-out event, saves the predictions as georeferenced GeoTIFFs, mosaics the predicted tiles for each event, and calculates standard segmentation metrics (IoU, Precision, Sensitivity, Specificity, F1, Accuracy) by comparing the mosaicked predictions against the ground truth labels. The training uses `fastai`'s `fine_tune` method.
    *   **Dynamic Paths:** The script dynamically constructs output paths for logs, model saves, predictions, and evaluation CSVs to include both the dataset identifier (`model_identifier`) and the model architecture (`model_arch`, set to `rexnet_150`).
    *   **Usage:**
        1.  Download and extract the dataset (see [Dataset](#dataset) section).
        2.  Crucially, update all hardcoded file paths defined at the beginning of `oneSinglePipeline.py` (e.g., `base_path`, `input_base_path`, `labels_base_path`, and `val_folder`).
        3.  Adjust CUDA device if necessary (`torch.cuda.set_device(1)`).
        4.  Modify hyperparameters (`epochs`, `lr`, `bs`) if desired.
        5.  Run the script from your terminal:
            ```bash
            python singlePipeline_timmModels.py
            ```
    *   **Outputs:**
        *   Training logs (`log_{model_identifier}_{model_arch}`).
        *   Trained models saved in the `models` subfolder within `input_base_path`, named `modelUFO_{model_identifier}_{leave_out.name}_{model_arch}`.
        *   Tiled predictions (GeoTIFFs) saved in subfolders under `output_folder` (`{model_identifier}_{model_arch}_Preds`).
        *   Mosaicked predictions (GeoTIFFs) saved in subfolders under `output_base` (`{model_identifier}_{model_arch}_Preds_ProcessedMosaics`).
        *   An evaluation results CSV file (`csv_output_path`) named `{model_identifier}_{model_arch}.csv`, containing metrics per file and an 'overall_mean'.

2.  **`validateUFO.py`:**
    *   **Purpose:** Performs a stand-alone evaluation on existing prediction GeoTIFFs.
    *   **Functionality:** Reads a directory of predicted inundation masks (e.g., the mosaicked predictions from `singlePipeline_timmModels.py`) and a corresponding directory of ground truth labels. It calculates per-file confusion matrices and standard segmentation metrics (Precision, Sensitivity, Specificity, F1, IoU, Accuracy). This script can be used to evaluate prediction sets generated by `singlePipeline_timmModels.py` or other methods, saving the results to a CSV file.
    *   **Usage:**
        1.  Ensure you have prediction mosaics and ground truth labels in accessible directories.
        2.  Update the input (`pred_folder`, `val_folder`) and output (`csv_output_path`) paths in the script to point to the specific files you want to evaluate and the desired output location/name.
        3.  Run the script:
            ```bash
            python validateUFO.py
            ```
    *   **Outputs:** An evaluation results CSV file containing metrics for each evaluated file/region, typically including an 'overall_mean'.

3.  **`plotValResultsUFO.py`:**
    *   **Purpose:** Generates summary plots from evaluation results CSVs.
    *   **Functionality:** Reads specific evaluation CSV file(s) (likely including columns like 'Region', 'Specificity', 'IoU', 'Filename', 'Accuracy_df1', 'Accuracy_df2', etc., depending on which plots are uncommented/active in the script). It calculates summaries (like mean specificity per region) and generates various plots using `seaborn` and `matplotlib`, such as boxplots visualizing metric distributions per region (Figure 7 in the paper) or comparing different models/sources (Figure 10), and potentially other visualizations like bar plots (Figure 2 placeholder) or map plots (Figure 1, requires GeoJSON and `geopandas`/`cartopy`). The plots often incorporate color coding based on mean metric values or model source.
    *   **Usage:**
        1.  Ensure you have the necessary evaluation results CSV file(s) generated by `singlePipeline_timmModels.py` or `validateUFO.py` (e.g., the `{model_identifier}_{model_arch}.csv` or specific comparison CSVs mentioned in the notebook snippets).
        2.  Update the input CSV file paths (`data_path`, `file_ps_s1`, `file_ps_dw`, `geojson_path` etc.) and output plot paths (`output_filename`) in the script.
        3.  Run the script:
            ```bash
            python plotValResultsUFO.py
            ```
    *   **Outputs:** Displays generated matplotlib/seaborn plots and saves figures to PDF files as configured in the script.

4.  **`UFO_figures.ipynb`:**
    *   **Purpose:** A Jupyter Notebook containing code cells to reproduce figures similar to those presented in the research manuscript.
    *   **Functionality:** This notebook integrates code snippets (similar to those provided) to load data from various sources (raw data characteristics, different evaluation CSVs comparing the PS-based RexNet model against other methods like Sentinel-1 IMPACT and Dynamic World, GeoJSON for map plots) and generate complex visualizations. This is likely where Figures 1, 2, 6, 7, 8, 9, 10, and potentially others from the paper are generated and analyzed.
    *   **Usage:**
        1.  Ensure you have completed the `singlePipeline_timmModels.py` run to generate necessary outputs (mosaicked predictions, the main evaluation CSV `{model_identifier}_{model_arch}.csv`).
        2.  Ensure you have any other required input files like comparison CSVs (e.g., `predictionsPSS1.csv`, `predictionsPSDW.csv`) or the GeoJSON file (`UFOEventsCentroids.geojson`).
        3.  Update all relevant data paths *within the notebook cells* to point to the specific files generated or required.
        4.  Open the notebook in a Jupyter environment (e.g., JupyterLab, VS Code, Google Colab) with the project dependencies installed and access to the data.
        5.  Run the code cells sequentially to regenerate the figures.
    *   **Outputs:** Displays generated figures within the notebook cells. Notebook cells also include commands to save figures to PDF files.

## Features

*   **Deep Learning Segmentation:** U-Net models based on the RexNet-150 encoder from the `timm` library.
*   **Leave-One-Event-Out Cross-Validation:** Robust training and evaluation method applied per urban flood event.
*   **Custom Data Handling:** Supports multi-channel GeoTIFF input images and masks using tailored `fastai` components compatible with `rasterio`.
*   **Custom Loss Function:** Employs a `CombinedLoss` (Focal Loss + Dice Loss) suitable for segmentation tasks, particularly with class imbalance.
*   **Georeferenced Predictions:** Saves predictions preserving spatial metadata using `rasterio`.
*   **Prediction Mosaicking:** Stitches predicted tiles into event/region-level mosaics using `rasterio.merge`.
*   **Quantitative Evaluation:** Calculates standard pixel-level segmentation metrics (Precision, Sensitivity, Specificity, F1, IoU, Accuracy) using confusion matrices.
*   **Data Analysis and Visualization:** Creating plots to visualize data characteristics, model performance distributions, comparisons between methods (including other widely used surface water products like Sentinel-1 IMPACT and Dynamic World), and spatial aspects.

## Dependencies

The core dependencies are:

*   `Python` (version compatible with libraries, likely 3.7+)
*   `pandas`
*   `numpy`
*   `torch`
*   `fastai`
*   `rasterio` (and its dependencies for GeoTIFF handling)
*   `scikit-image`
*   `scikit-learn`
*   `seaborn`
*   `matplotlib`
*   `geopandas`
*   `cartopy`
*   `squarify`
*   `timm` (PyTorch Image Models)

Ensure you have these installed before running the code.

## License

Creative Commons Attribution 4.0 International

## Acknowledgements

This project utilizes the `fastai` deep learning library, `PyTorch`, `timm` for accessing the RexNet architecture, `rasterio` for spatial data handling, and `pandas`, `seaborn`, and `matplotlib` for data analysis and visualization. `geopandas` and `cartopy` are used for geospatial plotting. The project is based on the Urban Flood Observation (UFO) dataset.

This work was supported by a NASA Terrestrial Hydrology Program grant (#80NSSC21K1044). 

We acknowledge the assistance with labeling from Simone Holliday, Patrick Hellmann, Natasha Rapp, and Linn Ji, and the contributions of all co-authors listed in the paper.
